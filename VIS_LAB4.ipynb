{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bilkouristas/oxf-vis-25/blob/master/VIS_LAB4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 4: 3D vision"
      ],
      "metadata": {
        "id": "WgmmRqUK6nIk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The purpose of this laboratory is to engage with the field of 3D vision through the practical application of 3D reconstruction techniques, using the [Acino dataset](https://github.com/African-Robotics-Unit/AcinoSet). This dataset, which comprises multi-view video of cheetahs in motion, presents a unique opportunity to explore advanced computer vision concepts in a real-world context. The lab's objectives are centered around the analysis of animal motion, with specific emphasis on computing the trajectory and velocity of cheetahs from the recorded videos."
      ],
      "metadata": {
        "id": "ZOhzF2KYGMbD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image](https://camo.githubusercontent.com/4a0f654a81cb2f1fdb233b7e697326de0b1036a80b3beb3e57ca956dfc21aa7f/68747470733a2f2f696d616765732e73717561726573706163652d63646e2e636f6d2f636f6e74656e742f76312f3537663664353163396637343536366635356563663237312f313630383437333235313335352d52364d44324450414758443534314f364b53504f2f6b6531375a77644742546f6464493870446d34386b444a695252696e76796c30696255524a634434326f4d55717378525571716272316d4f4a594b66495052374c6f4451396d58504f6a6f4a6f71793831533249384e5f4e34563176556235416f494949624c5a68565978435257344250753130537433544241555159564b63515268557845545257612d6f713134375474496f4337494959486358534576726d6c426f596d62724b4e5a5f474775696b38746163633450375f645f666e5f302f636865657461685475726e2e706e673f666f726d61743d3235303077)"
      ],
      "metadata": {
        "id": "v9ySG692moA7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset you will be working with includes multi-view video footage (`cam2.mp4`, `cam3.mp4`, etc.) and corresponding CSV files (`cam2_fte.csv`, `cam3_fte.csv`, etc.) that contain the frame-by-frame positions of these landmarks. Each row in a CSV file corresponds to a frame in the video, listing the (x, y) coordinates and a likelihood score indicating the confidence of each landmark's detection.\n",
        "\n",
        "For the this lab, we will give you the starter code for loading the data and plotting the cheetah's joints captured from two different camera views. For a given frame index, we then plot the corresponding points from two videos side by side to observe the motion from different perspectives."
      ],
      "metadata": {
        "id": "1hXYysfQHQZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import plotly.graph_objs as go\n",
        "from plotly.subplots import make_subplots"
      ],
      "metadata": {
        "id": "Xir4ljOKzFiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download(\"https://drive.google.com/uc?id=1DybvFguLonSU6v8vnKcwdoq2UMgBxWNt\", \"acino_dataset.zip\")\n",
        "!unzip \"acino_dataset.zip\""
      ],
      "metadata": {
        "id": "tiO68k1yJhps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "edCPdRVO-jxf"
      },
      "outputs": [],
      "source": [
        "def load_points_data(csv_file_path):\n",
        "    \"\"\"\n",
        "    Load points and their corresponding frame indices from a CSV file.\n",
        "    \"\"\"\n",
        "    points_df = pd.read_csv(csv_file_path, header=None, skiprows=2)\n",
        "    frame_indices = points_df.iloc[:, 0].values  # Frame indices\n",
        "    points_data = points_df.iloc[:, 1:].values  # Points data\n",
        "    return frame_indices, points_data\n",
        "\n",
        "def plot_points_on_frame(frame, points):\n",
        "    \"\"\"\n",
        "    Plot the given points on the frame.\n",
        "    \"\"\"\n",
        "    for i in range(0, points.shape[0], 3):  # Iterate through each set of 'x', 'y', 'likelihood'\n",
        "        x, y, likelihood = points[i], points[i+1], points[i+2]\n",
        "        # Plot with a red dot if likelihood meets threshold (example: 0.5)\n",
        "        plt.scatter(x, y, c='red', s=10)\n",
        "\n",
        "def plot_frame_with_points(video_path, frame_index, points):\n",
        "    \"\"\"\n",
        "    Plot points on the specified frame of a video.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
        "    success, frame = cap.read()\n",
        "    cap.release()\n",
        "\n",
        "    if not success:\n",
        "        print(f\"Failed to read frame at index {frame_index} from {video_path}.\")\n",
        "        return\n",
        "\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    plt.imshow(frame_rgb)\n",
        "    plot_points_on_frame(frame, points)\n",
        "\n",
        "\n",
        "def plot_points_for_frame_index(video1_path, csv1_path, video2_path, csv2_path, frame_index):\n",
        "    \"\"\"\n",
        "    Plot points for a specific frame index from two videos.\n",
        "    \"\"\"\n",
        "    frame_indices1, points_data1 = load_points_data(csv1_path)\n",
        "    frame_indices2, points_data2 = load_points_data(csv2_path)\n",
        "\n",
        "    index1 = np.where(frame_indices1 == frame_index)[0]\n",
        "    index2 = np.where(frame_indices2 == frame_index)[0]\n",
        "\n",
        "    if index1.size == 0 or index2.size == 0:\n",
        "        print(f\"Frame index {frame_index} not found in one of the videos.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(20, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plot_frame_with_points(video1_path, frame_index, points_data1[index1[0]])\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Video 1: Frame {frame_index}\")\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plot_frame_with_points(video2_path, frame_index, points_data2[index2[0]])\n",
        "    plt.axis('off')\n",
        "    plt.title(f\"Video 2: Frame {frame_index}\")\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can actually plot the cheetah keypoints in the view of camera 2 and camera 3. We start with `frame_index=200` but go ahead and change `frame_index` to some other frames to see how the cheetah and keypoints moves throughout the video."
      ],
      "metadata": {
        "id": "Xjxd9uEpyTMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "video1_path = '/content/acino_dataset/cam2.mp4'\n",
        "csv1_path = '/content/acino_dataset/cam2_fte.csv'\n",
        "video2_path = '/content/acino_dataset/cam3.mp4'\n",
        "csv2_path = '/content/acino_dataset/cam3_fte.csv'\n",
        "frame_index = 200  # Example frame index\n",
        "\n",
        "# You should see the same cheetah keypoints from two different camera viewpoints\n",
        "plot_points_for_frame_index(video1_path, csv1_path, video2_path, csv2_path, frame_index)"
      ],
      "metadata": {
        "id": "2j786LCayVQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above, we've seen what the keypoints look like when being plotted onto the cheetah. Here, you will need to implement the `get_points_for_frame` to obtain the specified 2D keypoints from the given CSV files and the particular frame_index. You can use `load_points_data` as a helper function."
      ],
      "metadata": {
        "id": "AsBJsa5M0UJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_points_for_frame(csv1_path, csv2_path, frame_index):\n",
        "    \"\"\"\n",
        "    Returns the points for a specific frame from two CSV files.\n",
        "\n",
        "    Parameters:\n",
        "    - csv1_path: Path to the first CSV file.\n",
        "    - csv2_path: Path to the second CSV file.\n",
        "    - frame_index: The frame index for which to retrieve the points.\n",
        "\n",
        "    Returns:\n",
        "    A tuple containing two numpy arrays: (points_view1, points_view2),\n",
        "    where each array is the points data for the specified frame from each view.\n",
        "    If the frame index is not found in one of the CSV files, None is returned for that view.\n",
        "    \"\"\"\n",
        "    # Load the points data using `load_points_data`\n",
        "\n",
        "    # Find the index of the requested frame in each dataset\n",
        "    # Hint: use np.where\n",
        "\n",
        "    # Obtain the actual 2D points located at the specified indices\n",
        "\n",
        "    return points_view1, points_view2"
      ],
      "metadata": {
        "id": "f-JhkItq3sTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having successfully visualized the 2D key points on the cheetah from multiple views, you will now advance to a crucial component of 3D vision: triangulation. Triangulation allows us to infer the three-dimensional position of a point by observing it from two different viewpoints. This process is fundamental in reconstructing the 3D structure of a scene from 2D images.\n",
        "\n",
        "Your task is to implement a function that triangulates the position of points on the cheetah from two different camera views. This will involve using camera calibration parameters, including intrinsic matrices (**K**), distortion coefficients (**D**), rotation matrices (**R**), and translation vectors (**T**), which are provided in a scene file which are provided in the calibration file of the dataset (`4_cam_scene_sba.json`).\n",
        "\n",
        "To save you from perusing the OpenCV documentation, we'll provide the following OpenCV function signatures. Familiarize yourself with these OpenCV functions, which will be essential for this task:\n",
        "\n",
        "- `cv2.undistortPoints`: Corrects radial and tangential distortion in the observed points.\n",
        "\n",
        "```\n",
        "def undistortPoints(distorted_pixel, K, distort_coeffs):\n",
        "\"\"\"\n",
        "Corrects radial and tangential distortion in the observed points.\n",
        "\n",
        "Params:\n",
        "    distorted_pts_pixel: (np.ndarray shaped Nx2) The distorted keypoints, expressed in *pixel space*.\n",
        "    K: (np.ndarray shaped 3x3) The intrinsic matrix.\n",
        "    distort_coeffs: (np.ndarray shaped 4x1) The distortion coefficients used to correct the radial distortion.\n",
        "\n",
        "Returns:\n",
        "    undistorted_pts_screen: (np.ndarray shaped Nx1x2) The undistorted keypoints, expressed in *screen space*.\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "- `cv2.triangulatePoints`: Triangulates the corresponding points from two views, given the projection matrices for each camera.\n",
        "\n",
        "```\n",
        "def triangulatePoints(proj_mat1, proj_mat2, undistorted_pts1, undistorted_pts2):\n",
        "Triangulates the corresponding points from two views, given the projection matrices for each camera.\n",
        "\n",
        "Params:\n",
        "    proj_mats1: (np.ndarray shaped 3x4) The projection matrix for camera 1.\n",
        "    proj_mats2: (np.ndarray shaped 3x4) The projection matrix for camera 2.\n",
        "    undistorted_pts1: (np.ndarray shaped 2xN) The undistorted keypoints in image 1.\n",
        "    undistorted_pts2: (np.ndarray shaped 2xN) The undistorted keypoints in image 2.\n",
        "Returns:\n",
        "    points_4d_homog: (np.ndarray shaped 4xN) The 4D homogenous keypoints expressed in the world frame.\n",
        "```\n"
      ],
      "metadata": {
        "id": "87kGL9Q3I8FS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have done this, implement the triangulation using the following steps as a guide:\n",
        "\n",
        "1. Extract Camera Calibration Parameters: We give the function `load_scene` in the provided `acino_calibration.py` file to load the camera calibration parameters from the scene file. You will then extract the camera extrinsics and intrinsics from this.\n",
        "\n",
        "2. Prepare Projection Matrices: Construct the projection matrices for each camera view by using the extrinsic parameters (rotation and translation).\n",
        "\n",
        "3. Correct Distortion: As the images were captured with a fisheye lens, they exhibit significant radial distortion. Apply cv2.undistortPoints to the 2D points from each view, using the respective camera's calibration parameters.\n",
        "\n",
        "4. Triangulate Points: With the undistorted points and projection matrices, use cv2.triangulatePoints to calculate the 3D coordinates of each point in a homogeneous coordinate system.\n",
        "\n",
        "5. Convert to 3D Coordinates: Convert the triangulated points from homogeneous coordinates to standard 3D coordinates."
      ],
      "metadata": {
        "id": "3LmQqDduwmaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from acino_dataset.acino_calibration import load_scene\n",
        "k_arr, d_arr, r_arr, t_arr, camera_res = load_scene('/content/acino_dataset/4_cam_scene_sba.json')"
      ],
      "metadata": {
        "id": "SUnVTikaIO1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def triangulatePoints(k_arr, d_arr, r_arr, t_arr, points_view1, points_view2):\n",
        "    # Extract the Ks, Rs, ts, and distortion coeffs\n",
        "    # We have 4 cameras in the stero setup but we want camera 2 and 3\n",
        "    # (Located at indices 1 and 2)\n",
        "    # Obtain the camera intrinsic matrices from k_arr\n",
        "\n",
        "    # Obtain the distortion coefficients [k1, k2, p1, p2, k3] from d_arr\n",
        "\n",
        "    # Obtain the camera extrinsics\n",
        "\n",
        "    # Compute the entire camera projection extrinsics\n",
        "\n",
        "    # Obtain the corresponding points in both images, from points_view1 and points_view2, how can we extract 2D x,y coords?\n",
        "\n",
        "    # Convert points to the shape expected by cv2.undistortPoints (2xN to Nx1x2)\n",
        "\n",
        "    # Correct radial distortion using cv2.undistortPorts\n",
        "\n",
        "    # Triangulate points using cv2.triangulatePoints\n",
        "\n",
        "    # Convert from homogeneous coordinates to 3D\n",
        "\n",
        "    return points_3d"
      ],
      "metadata": {
        "id": "hDt7K00mAq1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame_index = 200\n",
        "points_view1, points_view2 = get_points_for_frame(csv1_path, csv2_path, frame_index)\n",
        "triangulatePoints(k_arr, d_arr, r_arr, t_arr, points_view1, points_view2)\n",
        "\n",
        "frame_index = 240\n",
        "points_view1, points_view2 = get_points_for_frame(csv1_path, csv2_path, frame_index)\n",
        "triangulatePoints(k_arr, d_arr, r_arr, t_arr, points_view1, points_view2)"
      ],
      "metadata": {
        "id": "yzDyADNBCY_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After successfully triangulating the cheetah's 2D keypoint detections from multiple camera views and converting them to 3D, we are now in a position to analyse the cheetah's motion. This final part of the lab focuses on applying the triangulation process across multiple frames, plotting the resulting 3D trajectory, and computing the velocity of the cheetah over the sequence. This exercise encapsulates the essence of 3D motion analysis.\n",
        "\n",
        "Triangulate Points Across Multiple Frames: Implement a loop or a function that applies the triangulation process to a sequence of frames. This will involve extracting and triangulating the corresponding points from each frame and accumulating the 3D coordinates.\n",
        "\n"
      ],
      "metadata": {
        "id": "OzZ_Rd8hKcFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_trajectory(csv1_path, csv2_path):\n",
        "    # return the trajectory of the chosen keypoints as a TxNx3 numpy array\n",
        "    # where T=length of the trajectory, N=number of keypoints\n"
      ],
      "metadata": {
        "id": "RvAqKPJwC6J0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the 3D Trajectory: Utilize a 3D plotting library (such as matplotlib's 3D plotting capabilities) to visualize the path of the cheetah. Each set of triangulated 3D points should be plotted in sequence, illustrating the trajectory over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "kv__tViKLCbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_trajectory(trajectory, velocity=None):\n",
        "    # plot the TxNx3 numpy array as a 3D trajectory\n",
        "    traj_len = trajectory.shape[0]\n",
        "    neck_points = trajectory[:,3,:]\n",
        "    spine_points = trajectory[:,4,:]\n",
        "    tail_points = trajectory[:,5,:]\n",
        "\n",
        "    keypoints_dict = {\n",
        "        \"x\": np.hstack((neck_points[:, 0], spine_points[:, 0], tail_points[:, 0])),\n",
        "        \"y\": np.hstack((neck_points[:, 1], spine_points[:, 1], tail_points[:, 1])),\n",
        "        \"z\": np.hstack((neck_points[:, 2], spine_points[:, 2], tail_points[:, 2])),\n",
        "        \"body_part\": [\"neck\"]*traj_len + [\"spine\"]*traj_len + [\"tail\"]*traj_len,\n",
        "        \"index\": np.tile(np.arange(traj_len), 3)\n",
        "\n",
        "    }\n",
        "    if velocity is not None:\n",
        "        velocity = np.concatenate((velocity[0][None,:,:], velocity), axis=0)\n",
        "        neck_vel = velocity[:,3,:]\n",
        "        spine_vel = velocity[:,4,:]\n",
        "        tail_vel = velocity[:,5,:]\n",
        "        keypoints_dict[\"velocity\"] = np.hstack((neck_vel[:, 0], spine_vel[:, 0], tail_vel[:, 0]))\n",
        "\n",
        "    keypoints_df = pd.DataFrame(keypoints_dict)\n",
        "    fig = go.Figure()\n",
        "    if velocity is not None:\n",
        "        fig = px.scatter_3d(keypoints_df, x=\"x\", y=\"y\", z=\"z\", color=\"velocity\",\n",
        "                            hover_name=\"body_part\", color_continuous_scale=\"agsunset\",\n",
        "                            range_x=[0, 12], range_y=[0, 12], range_z=[0, 1])\n",
        "    else:\n",
        "        fig = px.scatter_3d(keypoints_df, x=\"x\", y=\"y\", z=\"z\", color=\"index\",\n",
        "                            hover_name=\"body_part\", color_continuous_scale=\"agsunset\",\n",
        "                            range_x=[0, 12], range_y=[0, 12], range_z=[0, 1])\n",
        "    fig.update_traces(marker=dict(size=3))\n",
        "    fig.show()\n",
        "\n",
        "plot_trajectory(trajectory)"
      ],
      "metadata": {
        "id": "LBGgjab_LK27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "num_keypoints = 20\n",
        "num_frames = 200\n",
        "\n",
        "# Add initial state (first frame)\n",
        "for i in range(num_keypoints):\n",
        "    fig.add_trace(go.Scatter3d(x=[trajectory[0, i, 0]],\n",
        "                               y=[trajectory[0, i, 1]],\n",
        "                               z=[trajectory[0, i, 2]],\n",
        "                               mode='markers',\n",
        "                               name=f'Keypoint {i+1}'))\n",
        "\n",
        "# Create frames for each time step\n",
        "frames = [go.Frame(data=[go.Scatter3d(x=trajectory[frame, :, 0],\n",
        "                                      y=trajectory[frame, :, 1],\n",
        "                                      z=trajectory[frame, :, 2],\n",
        "                                      mode='markers',\n",
        "                                      marker=dict(size=5, symbol='circle'))\n",
        "                         for _ in range(num_keypoints)])\n",
        "          for frame in range(1, num_frames)]\n",
        "\n",
        "fig.frames = frames\n",
        "\n",
        "# Layout settings\n",
        "fig.update_layout(updatemenus=[dict(type=\"buttons\",\n",
        "                                    buttons=[dict(label=\"Play\",\n",
        "                                                  method=\"animate\",\n",
        "                                                  args=[None, {\"frame\": {\"duration\": 50}, \"fromcurrent\": True}]),\n",
        "                                             dict(label=\"Pause\",\n",
        "                                                  method=\"animate\",\n",
        "                                                  args=[[None], {\"frame\": {\"duration\": 0}, \"mode\": \"immediate\"}])])],\n",
        "                  title='Keypoint Trajectory Animation')\n",
        "\n",
        "# Show the figure\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "cs14Hu0GCbBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Velocity Computation: Calculate the cheetah's velocity between consecutive frames. This can be achieved by computing the distance between points in successive frames and dividing by the time interval between those frames. Consider the frame rate of the video to determine the time interval."
      ],
      "metadata": {
        "id": "M3NhylndLLHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_velocity():\n",
        "  # plot the TxNx3 numpy array as a 3D trajectory"
      ],
      "metadata": {
        "id": "v5tLMHRnLdsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_trajectory(trajectory, velocity=vel)"
      ],
      "metadata": {
        "id": "jxXwr_edp9vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus tasks:\n",
        "\n",
        "**Data Smoothing**: The raw triangulation data may contain noise or inaccuracies due to measurement errors or limitations in the detection algorithm. Try to apply temporal smoothing or filtering techniques to the 3D trajectory data to achieve more accurate velocity calculations.\n",
        "\n",
        "**Visualization Enhancements**: Enhance your 3D trajectory plot with features such as color gradients to indicate velocity"
      ],
      "metadata": {
        "id": "9AjGxB7HLlZx"
      }
    }
  ]
}